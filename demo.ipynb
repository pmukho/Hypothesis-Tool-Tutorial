{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "This notebook serves as a demo of the basic features and functionality of the Python tool **Hypothesis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Hypothesis\n",
    "\n",
    "Hypothesis is a tool that allows us to write and generate tests that generate and allow for fuzzing over a large variety of data without actually manually making all the tests. The key features mentioned are **property-based-testing** using `@given`, **strategies** for primitive and more complex data types, **shrinking** to find the simplest possible counter-example, and **stateful testing** to test with defined rules and checking invariants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Usage: `@given` and Strategies\n",
    "\n",
    "The core of Hypothesis is the `@given` decorator, which takes **strategies** that define how to generate data. Let's test a simple `add` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis import given, strategies as st\n",
    "\n",
    "def add(a, b):\n",
    "    \"\"\"A simple function to add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# This is a property-based test!\n",
    "# It states that for any two integers a and b, add(a, b) should equal a + b.\n",
    "\n",
    "@given(a=st.integers(), b=st.integers())\n",
    "def test_addition_property(a, b):\n",
    "    print(f\"Testing with a={a}, b={b}\") \n",
    "    assert add(a, b) == a + b\n",
    "\n",
    "test_addition_property()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how Hypothesis tries a wide range of integers: positives, negatives, and zero. If the assertion had failed, Hypothesis would have reported it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shrinking, Counter-Examples, and Replaying\n",
    "\n",
    "Hypothesis's superpower is finding the *simplest possible* failing example. This is called **shrinking**. Let's introduce a bug into a sorting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree('.hypothesis')\n",
    "except:\n",
    "    print('no cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis import find, settings, note\n",
    "from typing import List\n",
    "\n",
    "def buggy_sort(numbers: List[int]) -> List[int]:\n",
    "    if 0 in numbers and 1 in numbers:\n",
    "        return [999] #oh no our sort is wrong\n",
    "    return sorted(numbers)\n",
    "\n",
    "# defined strategy\n",
    "list_strategy = st.lists(\n",
    "    st.integers(min_value=-5, max_value=5),\n",
    "    min_size=1,\n",
    "    max_size=10\n",
    ")\n",
    "\n",
    "# ignore\n",
    "seen_inputs = []\n",
    "\n",
    "#ignore\n",
    "def check_sort(numbers: List[int]):\n",
    "    seen_inputs.append(numbers[:])  \n",
    "    note(f\"Checking example: {numbers}\")\n",
    "    assert buggy_sort(numbers) == sorted(numbers)\n",
    "\n",
    "# --- Settings ---\n",
    "@settings(verbosity=2, max_examples=1000, deadline=None)\n",
    "def run_find():\n",
    "    print(\"Searching for example\")\n",
    "    failing_example = find(list_strategy, check_sort)\n",
    "    print(f\" {failing_example}\")\n",
    "\n",
    "run_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sizes = [len(x) for x in seen_inputs]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sizes, marker='o', linestyle='-', color='blue', label='Input size')\n",
    "plt.xlabel('Test number')\n",
    "plt.ylabel('Input size (length of list)')\n",
    "plt.title(' Shrinking Behavior in Hypothesis')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might note it shrank it down to the simplest possible case that still causes the error.\n",
    "\n",
    "#### Replaying Failures\n",
    "\n",
    "When a test fails, Hypothesis saves the failing example. You can use the `@example` decorator to add it as a permanent regression test case, ensuring the bug never comes back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis import given, example\n",
    "\n",
    "\n",
    "@given(st.lists(st.integers()))\n",
    "@example([0, 1])  \n",
    "def test_buggy_sort(numbers):\n",
    "    try:\n",
    "        assert buggy_sort(numbers) == sorted(numbers)\n",
    "    except AssertionError:\n",
    "        print(f\"Test failed for input: {numbers}\")\n",
    "\n",
    "test_buggy_sort()\n",
    "\n",
    "print(\"Test with @example ran.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Hypothesis database may be invalidated when using different Hypothesis versions or when changing the source code of test functions. Instead of relying on the Hypothesis to replay the last failure from the database, it may be preferred to use `@example` in these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and Transformations (`.filter()`, `assume`, `.map()`) â€” **Pritam**\n",
    "\n",
    "Hypothesis provides several ways to narrow down the set of possible examples to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis import assume\n",
    "\n",
    "count_examples = {\"filter\": 0, \"assume\": 0, \"early return\": 0}\n",
    "count_pass = {\"filter\": 0, \"assume\": 0, \"early return\": 0}\n",
    "\n",
    "@settings(max_examples=50)\n",
    "@given(st.integers().filter(lambda x: x % 2 == 0), st.integers().filter(lambda x: x % 2 == 0))\n",
    "def test_filter(a, b):\n",
    "    count_examples[\"filter\"] += 1\n",
    "\n",
    "    assert (a+b) % 2 == 0\n",
    "\n",
    "    count_pass[\"filter\"] += 1\n",
    "\n",
    "@settings(max_examples=50)\n",
    "@given(st.integers(), st.integers())\n",
    "def test_assume(a, b):\n",
    "    count_examples[\"assume\"] += 1\n",
    "\n",
    "    assume(a % 2 == 0 and b % 2 == 0)\n",
    "    assert (a+b) % 2 == 0\n",
    "\n",
    "    count_pass[\"assume\"] += 1\n",
    "\n",
    "@settings(max_examples=50)\n",
    "@given(st.integers(), st.integers())\n",
    "def test_early_return(a, b):\n",
    "    count_examples[\"early return\"] += 1\n",
    "\n",
    "    if a % 2 != 0 or b % 2 != 0:\n",
    "        return\n",
    "    assert (a+b) % 2 == 0\n",
    "\n",
    "    count_pass[\"early return\"] += 1\n",
    "\n",
    "test_filter()\n",
    "test_assume()\n",
    "test_early_return()\n",
    "\n",
    "print(f\"{'method':15s} | {'examples created':20s} | {'success rate':>11s}\")\n",
    "print(\"=\"*60)\n",
    "for method in \"filter\", \"assume\", \"early return\":\n",
    "    print(f\"{method:15s} | {count_examples[method]:20d} | {count_pass[method]:10d}/50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@settings(max_examples=500)\n",
    "@given(st.integers().map(lambda x: x*2), st.integers().map(lambda x: x*2))\n",
    "def test_mapping(a, b):\n",
    "    assert (a+b) % 2 == 0\n",
    "\n",
    "@settings(max_examples=500)\n",
    "@given(st.integers().filter(lambda x: x % 2 == 0), st.integers().filter(lambda x: x % 2 == 0))\n",
    "def test_filtering(a, b):\n",
    "    assert (a+b) % 2 == 0\n",
    "\n",
    "for method, fn in (\"mapping\", test_mapping), (\"filtering\", test_filtering):\n",
    "    start_time = time.perf_counter()\n",
    "    fn()\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"{method:8s} | time: {elapsed:8.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "1. A successful example is one where an assert does not fail which is why early return generates exactly 50 examples that are not considered failures.\n",
    "2. `.assume()` will not make progress towards max examples if assumption fails, but will still invoke the function (think about side effects if assumption is in middle of body)\n",
    "3. Should prefer `.filter()` to `.assume()` and avoid early return when using Hypothesis.\n",
    "4. Consider using mapping over filtering for potential perfomance gains.\n",
    "\n",
    "Hypothesis also claims that simple `.filter()` can be sometimes be optimized beyond trivial rejection sampling unlike `.assume()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom and Composite Strategies\n",
    "\n",
    "What if you need to generate custom objects? The `@composite` decorator lets you build new strategies from existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis.strategies import composite\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class User:\n",
    "    id: int\n",
    "    name: str\n",
    "\n",
    "@composite\n",
    "def user_strategy(draw):\n",
    "    \"\"\"A strategy to generate User objects.\"\"\"\n",
    "    # `draw` works like `data.draw()` from the previous example\n",
    "    user_id = draw(st.integers(min_value=1))\n",
    "    user_name = draw(st.text(min_size=3, max_size=20))\n",
    "    return User(id=user_id, name=user_name)\n",
    "\n",
    "@given(user=user_strategy())\n",
    "def test_user_creation(user):\n",
    "    print(f\"Generated User: {user}\")\n",
    "    assert isinstance(user, User)\n",
    "    assert user.id > 0\n",
    "    assert len(user.name) >= 3\n",
    "\n",
    "test_user_creation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stateful Testing\n",
    "\n",
    "Hypothesis can even test stateful systems by generating sequences of actions and checking that invariants (rules that should always be true) hold.\n",
    "\n",
    "Let's test a simple `SimpleStack` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis.stateful import RuleBasedStateMachine, rule, precondition\n",
    "\n",
    "class SimpleStack:\n",
    "    \"\"\"A basic stack implementation.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._items = []\n",
    "    \n",
    "    def push(self, item):\n",
    "        self._items.append(item)\n",
    "        \n",
    "    def pop(self):\n",
    "        if not self._items:\n",
    "            raise IndexError(\"pop from empty list\")\n",
    "        return self._items.pop()\n",
    "    \n",
    "    def is_empty(self):\n",
    "        return not self._items\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self._items)\n",
    "\n",
    "class StackStateMachine(RuleBasedStateMachine):\n",
    "    \"\"\"Defines the rules for testing our stack.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stack = SimpleStack()\n",
    "        self.model = [] # A simple list to model the stack's behavior\n",
    "        \n",
    "    @rule(item=st.integers())\n",
    "    def push_item(self, item):\n",
    "        \"\"\"Rule for pushing an item.\"\"\"\n",
    "        self.stack.push(item)\n",
    "        self.model.append(item)\n",
    "        print(f\"Pushed {item}\")\n",
    "        \n",
    "    @rule()\n",
    "    @precondition(lambda self: not self.stack.is_empty()) # Only run pop if not empty\n",
    "    def pop_item(self):\n",
    "        \"\"\"Rule for popping an item.\"\"\"\n",
    "        popped_stack = self.stack.pop()\n",
    "        popped_model = self.model.pop()\n",
    "        print(f\"Popped {popped_stack}\")\n",
    "        assert popped_stack == popped_model\n",
    "    \n",
    "    @rule()\n",
    "    def check_invariants(self):\n",
    "        \"\"\"This rule checks properties that should always be true.\"\"\"\n",
    "        print(\"Checking invariants...\")\n",
    "        assert self.stack.size == len(self.model)\n",
    "        assert self.stack.is_empty() == (not self.model)\n",
    "\n",
    "# To run this, Hypothesis would execute a sequence of the rules defined above.\n",
    "TestStack = StackStateMachine.TestCase\n",
    "TestStack.runTest = lambda self: None # Suppress unittest's default runner\n",
    "\n",
    "test_case = TestStack()\n",
    "test_case.execute_step = test_case.execute_step\n",
    "print(\"Running stateful test...\")\n",
    "# Manually run a few steps for demonstration\n",
    "try:\n",
    "    for i in range(10): # Run 10 random steps\n",
    "        test_case.execute_step(test_case.steps.pop(0))\n",
    "except (IndexError, AttributeError):\n",
    "    pass # Stop if we run out of generated steps\n",
    "print(\"\\nStateful test finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
